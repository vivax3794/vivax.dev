# Compiler part 2.1 - Cli and parsing

## Intro

> Just a fair warning: This post is *very* long, Since it details the entire structure of the parser it takes a while.

So let's start by deleting the code we wrote last blog post! Yhe, Yhe I know, but it was just a proof of concept. now we are gonna write some real code! 

> Also forgot to mention this in the last post, the inkwell docs dont build correctly on docs.rs, use <https://thedan64.github.io/inkwell/inkwell/index.html> instead.

I will also now be publishing the code on GitHub since we will be getting into a lot of files, should you ever be confused about where the code goes you can check there. I will use the same repo for each post, but I will link to specific commits in the posts (if you are unsure what version you are viewing the README should say).

<https://github.com/vivax3794/viv_script_blog/tree/211fb45ae1b6ca1b30e96842cd340fb03eabbed6>

The parts we will build today will work to support print statements of numbers, and that is all, in future posts, we will expand on this!
```
print 1;
print 2;
print 3;
```

We will also have to decide, should we have multiple int types? I think we should have Both Float and Int, Float should be `f64` and an Int should be `i32`, and we should have an implicit conversion from Int to Float (unlike `Rust`), but for now we shall only implement Int. 

So to make this easier to change later we going to define some type aliases in `lib.rs`
```rust
// lib.rs

type IntType = i32;
type FloatType = f64;

const IntWidth: usize = 32;
const FloatWidth: usize = 64;
```

The type aliases are for use by our code in stuff like the IR and AST, while the `const`s are for the code generation! 

## CLI

For our command line interface we are going to use [`clap`](https://crates.io/crates/clap), it is a fairly simple cli library, which is perfect for us. We are also going to [`anyhow`](https://crates.io/crates/anyhow) and [`thiserror`](https://crates.io/crates/thiserror) for error handling.
```bash
carg add clap --features derive
cargo add anyhow
cargo add thiserror
```

We are going to put the CLI handling in `main.rs`, and the rest of the code in `lib.rs` (and sub-modules). 

### Run

So for now we just gonna have the `run` command, in the future we will also have a `test` command. 

The interface will look like this:
```bash
our_compiler run input_file.viv
```
Well since we are running it via `cargo run` we will use `--` to pass arguments to our code.
```bash
cargo run -- run input_file.viv
```

We will define an enum that will hold out different commands.
```rust
use clap::{Parser, Subcommand};

#[derive(Subcommand)]
enum CompilerCommand {
    /// Compile and run file
    Run { input_file: String },
}

#[derive(Parser)]
struct CompilerArguments {
    #[command(subcommand)]
    command: CompilerCommand,
}
```
so `CompilerCommand` will be our sub-commands, for now just `run`, then `CompilerArguments` will be the struct will all the CLI info, including future flags we will add.

We now need to actually parse the arguments, which is really simple. 
```rust
fn main() -> anyhow::Result<()> {
    let arguments = CompilerArguments::parse();

    match arguments.command {
        CompilerCommand::Run { input_file } => {
            println!("file: {input_file:?}");
        }
    }

    Ok(())
}
```
So we return a `anyhow::Result`, which is just a type alias for `Result<..., anyhow:Error>`, you might not know this but when you return an error from `main()` rust will print it to stderr? Next, we call the `parse` function from our Parser, this is provided by deriving `Parser`.

Now Clap gives us a lot of stuff for free! Like help messages:

![--help messages](/assets/compiler/cli/help_messages.png)

And ofc we can run our command! 

![Run result](/assets/compiler/cli/run_result.png)

We also get good error messages!

![Error messages](/assets/compiler/cli/error_message.png)

### Debug Flags

We are also going to have some Debug flags, these are to help us when building the compiler!
```rust
#[derive(Args)]
struct DebugArguments {
    /// Turn off compiler optimizations
    #[arg(short = 'd', long, global = true)]
    dont_optimize: bool,

    /// Print the tokens to stdout
    #[arg(short = 't', long, global = true)]
    output_tokens: bool,

    /// Print the ast to stdout
    #[arg(short = 'a', long, global = true)]
    output_ast: bool,

    /// Print the Internal ir to stdout
    #[arg(short = 'i', long, global = true)]
    output_ir: bool,

    /// Print the produced LLVM ir to stdout
    #[arg(short = 'l', long, global = true)]
    output_llvm: bool,
}

#[derive(Parser)]
struct CompilerCli {
    #[command(subcommand)]
    command: CompilerCommand,

    #[command(flatten)]
    debug: DebugArguments,
}
```
This will let us debug our compiler, and will be useful to make sure stuff is working as we progress thru the implementation of the compiler! The `global = true` means that these can be used anywhere on the command line, not just at the start, so `-t run test.viv` and `run -t test.viv` are the same.

We will also need to set up some basic stuff in `lib.rs`
```rust
// lib.rs
pub struct CompilerOptions {
    pub dont_optimize: bool,
    pub output_tokens: bool,
    pub output_ast: bool,
    pub output_ir: bool,
    pub output_llvm: bool,
}

pub fn build(file: &str, options: CompilerOptions) -> anyhow::Result<()> {}
```
Wait? Didn't we already define those compiler options? ...Well yes, but the reason we are doing this is that you cant import from `main.rs` to `lib.rs`.
You might then be asking yourself, why not just move `DebugArguments` into `lib.rs`? Well, we totally could! And you can do that if you want! Why aren't I? Well, `DebugArguments` has CLI stuff in it, and personally, I want to keep that stuff in `main.rs`.

We will then construct these Compiler options and pass them to `build`
```rust
// main.rs
use anyhow::Context;
use viv_script::{build, CompilerOptions};

// ...

fn main() -> anyhow::Result<()> {
    let arguments = CompilerCli::parse();
    let compiler_options = CompilerOptions {
        dont_optimize: arguments.debug.dont_optimize,
        output_tokens: arguments.debug.output_tokens,
        output_ast: arguments.debug.output_ast,
        output_ir: arguments.debug.output_ir,
        output_llvm: arguments.debug.output_llvm,
    };

    match arguments.command {
        CompilerCommand::Run { input_file } => {
            build(input_file, compiler_options).context("Compiling input file")?;
        }
    }

    Ok(())
}
```
> Do note that `viv_script` should be replaced with your project name, in the other files we will be using `crate` meaning the name doesnt matter.

## Parsing
Now that we got the CLI out of the way let's start doing some parsing!

Parsing is the work of taking text and turning it into a structure that makes sense and is easy to work with!
We are gonna do this in two steps, first, we tokenize the text, then we parse it. 
Tokenizing basically takes the text and groups it into chunks that are easier to work with. 

Let's create a `parsing` module
```
src/
  parsing/
    mod.rs  <-- Create this file!
  lib.rs
  main.rs
```

And ofc we need to add it to `lib.rs` (in the future I will just assume you do this when we create a new file)
```rust
// lib.rs

mod parsing;
```

### Ast

Now let's design our AST first, we will create an `ast.rs` module (in `parsing/), and we will also make the module public.
```rust
// parsing/mod.rs

pub mod ast;
```

So for our AST, we could have a `Print` struct and a `Number` struct, BUT we need to think about the future, so instead we will have `Statement`, `Expression`, and `Literal`!

```rust
/// parsing/ast.rs

use crate::IntType;

#[derive(Debug)]
pub struct Module(pub Vec<Statement>);

#[derive(Debug)]
pub enum Statement {
    Print(Expression),
}

#[derive(Debug)]
pub enum Expression {
    Literal(Literal),
}

#[derive(Debug)]
pub enum Literal {
    Integer(IntType),
}
```

So a `Module` is a list of Statements, and here we use the type alias we set up at the start, so if you decide that you want bigger or smaller ints you can do that easily in `lib.rs`!

```
print 1;
print 2;
```
Would be parsed into
```rust
Module(vec![
  Statement::Print(Expression::Literal(Literal::Integer(1))),
  Statement::Print(Expression::Literal(Literal::Integer(2)))
])
```

### Tokens

Okay let's leave the Ast for and focus on tokens, these should be really small groups of meaningful data.
Consider the following code:
```
print 1 + 3;
```
This might be tokenized into:
```rust
PRINT, NUMBER(1), PLUS, NUMBER(3), SEMICOLON, EOF
```
the tokens are basically how you would read out the code if you were telling somebody how to write it!


### Tokenizer

Let's make a `parsing/tokens.rs` file, (this one doesn't need to be public from `parsing/mod.rs`).
We will declare the tokens and tokenizer in the same file as they aren't that complex, while the Ast and Parser we split up because of their complexities (and also the parser doesn't need to be known outside `parsing` while the Ast does)

We will declare the tokens we need so far!
```rust
use crate::IntType;

#[derive(Debug, PartialEq, Eq)]
pub enum TokenType {
    Integer(IntType),
    Identifier(String),
    Print,
    SemiColon,
    Eof,
}

pub struct Token {
    pub _type: TokenType,
    pub line: usize,
    pub char: usize,
}
```
You may be asking, what is this `Identifier` about? Well, we aren't going to use it (yet!), but it makes the tokenizing logic easier to have it now, in the future this will be used for variables! 

We will also declare an error for the tokenizer so that we can get error messages for unknown chars!

```rust
use thiserror::Error;

#[derive(Error, Debug)]
#[error("Tokenizer error on line {line}, char {char}: {message}")]
pub struct TokenizerError {
    pub line: usize,
    pub char: usize,
    pub message: String,
}
```

Now we need to write the tokenizer, but first! I actually want to write a generic class in `parsing/mod.rs` as both the tokenizer and Parser are gonna use the same pattern!

```rust
// parsing/mod.rs

use std::collections::VecDeque;
use thiserror::Error;

pub mod ast;
mod tokens;

#[derive(Error, Debug)]
#[error("Stream ran out of elements")]
struct StreamerError;

struct StreamConsumer<T>(VecDeque<T>);

impl<T> StreamConsumer<T> {
    fn new(stream: VecDeque<T>) -> Self {
        Self(stream)
    }

    fn advance(&mut self) -> Result<T, StreamerError> {
        self.0.pop_front().ok_or(StreamerError)
    }

    fn void(&mut self) {
        self.0.pop_front();
    }

    fn peek(&self) -> Result<&T, StreamerError> {
        self.0.front().ok_or(StreamerError)
    }
}
```

So this class will be used by the tokenizer to consume chars, and by the parser to consume tokens. it lets us consume elements, look at what the next element is, and also ignore a value (useful after a peek). We also provide a general stream error!

Now that we got that implemented, let's create the tokenizer!

```rust
// parsing/tokens.rs

pub struct Tokenizer {
    code: StreamConsumer<char>,
    line: usize,
    char: usize,
}

impl Tokenizer {
    pub fn new(code: &str) -> Self {
        Self {
            code: StreamConsumer::new(code.chars().collect()),
            line: 1,
            char: 1,
        }
    }

    fn error(&self, msg: String) -> Result<(), TokenizerError> {
        Err(TokenizerError {
            line: self.line,
            char: self.char,
            message: msg,
        })
    }

    fn token(&self, _type: TokenType) -> Token {
        Token {
            _type,
            line: self.line,
            char: self.char,
        }
    }

    fn advance(&mut self) -> Option<char> {
        let c = self.code.advance().ok()?;
        if c == '\n' {
            self.line += 1;
            self.char = 1;
        } else {
            self.char += 1;
        }
        Some(c)
    }

    fn void(&mut self) {
        self.advance();
    }
}
```
It is a good amount of code, and that isn't all of it, this is just the utility functions we will be using! They should all be pretty self-explanatory.

Also, you may ask, why are we using `self.advance` instead of `self.code.void`? Simple! `advance` keeps track of our location ;)

```rust
    fn consume_number(&mut self) -> Token {
        let mut number = String::new();
        while let Ok(c) = self.code.peek() {
            if c.is_ascii_digit() {
                number.push(*c);
                self.void();
            } else {
                break;
            }
        }
        self.token(TokenType::Integer(number.parse().unwrap()))
    }
```
We will call this when we hit a number, it will keep eating characters until it hits the end or a non-number char.
But wait, is that `.unwrap()` I spot?! I thought you said we were gonna have good error handling! 
But we are! In this case, since every char was checked with `.is_ascii_digit` we know that `number` must be valid :D 

```rust
    fn consume_identifier(&mut self) -> Token {
        let mut identifier = String::new();
        while let Ok(c) = self.code.peek() {
            if c.is_ascii_alphanumeric() {
                identifier.push(*c);
                self.void();
            } else {
                break;
            }
        }

        self.token(match identifier.as_str() {
            "print" => TokenType::Print,
            _ => TokenType::Identifier(identifier),
        })
    }
```
This code is similar, we will call this on any alphabetic character, and identifiers can contain numbers in them.
After we read the identifier we check if it is a keyword (in this case we only got "print"), if not we create an Identifier.

```rust
    fn consume_whitespace(&mut self) {
        while let Ok(c) = self.code.peek() {
            if c.is_ascii_whitespace() {
                self.void();
            } else {
                break;
            }
        }
    }
```
This will let us remove whitespace easily, and this is why tokenizing is so useful, not only do we get identifiers and numbers grouped up when parsing, but we also don't have to deal with whitespace! 

```rust
    fn consume_comment(&mut self) {
        while let Ok(c) = self.code.peek() {
            if c == &'\n' {
                self.void();
                break;
            } else {
                self.void();
            }
        }
    }
```
We will use `#` for comments, and ofc we just ignore it, another reason tokenizing is useful, the parser doesn't have to care about comments!

```rust
    pub fn tokenize(mut self) -> Result<Vec<Token>, TokenizerError> {
        let mut tokens = Vec::new();
        while let Ok(&c) = self.code.peek() {
            match c {
                c if c.is_ascii_digit() => tokens.push(self.consume_number()),
                c if c.is_ascii_alphabetic() => tokens.push(self.consume_identifier()),
                ';' => {
                    tokens.push(self.token(TokenType::SemiColon));
                    self.void();
                }
                c if c.is_ascii_whitespace() => self.consume_whitespace(),
                '#' => self.consume_comment(),
                _ => {
                    self.error(format!("Unexpected character: {}", c))?;
                }
            }
        }
        tokens.push(self.token(TokenType::Eof));
        Ok(tokens)
    }
```
And here is our main function, it is rather straightforward when we got those other functions to do the hard work.
* is it a digit? consume a number
* is it a letter? consume an identifier
* is it a semicolon? create a semicolon
* is it whitespace? Consume and ignore
* Anything else? ERROR
* And ofc an `Eof` at the end!

> Also you might wonder, why are we taking ownership of `self`? simple, this struct wont be useful anymore once the stream is consumed, so to prevent errors we take ownership! 

Now we can start writing the `parse` function in `parsing/mod.rs`, soon we will also add the parsing step to this
```rust
// parsing/mod.rs

pub fn parse(code: &str, compiler_options: &CompilerOptions) -> anyhow::Result<ast::Module> {
    let tokenizer = tokens::Tokenizer::new(code);
    let tokens = tokenizer.tokenize()?;

    if compiler_options.output_tokens {
        // we dont care about position info in this output
        let token_types = tokens
            .iter()
            .map(|t| format!("{:?}", t._type))
            // This is inefficient! itertools has a more efficient join method, but I dont want to add a dependency just for that
            // You however can :D
            .collect::<Vec<String>>()
            .join(", ");

        println!("TOKENS: {token_types}");
    }

    todo!();
}
```
Here we also use the compiler flags to maybe print out the tokens! 
We also have a `todo!()` at the end so Rust won't complain about our return type :D 

Now we can add the `parse` call to our `build` function,
```rust
use anyhow::Context;

// ...

pub fn build(file: &str, options: CompilerOptions) -> anyhow::Result<()> {
    let ast = parsing::parse(&file, &options).context("Parsing input file")?;

    Ok(())
}
```

And now we can actually test our code so far, let's write a test script
```
# test.viv
print 1;
print 2;
```
Now we can check what tokens we get!
```bash
cargo run -- run test.viv --output-tokens 
```
You should get an output like
```rust
Print, Integer(1), SemiColon, Print, Integer(2), SemiColon, Eof
```
(You can ignore the panic, that is from the `todo!()`)

We can also test some error handling now!
Try specifying a wrong file name, you should get a helpful error :D 
Or add a symbol we haven't supported yet to the file!

Arent those errors just amazing? Always remember to test your error handling!

### Parser

Now the more fun part starts! Writing a parser, we can do a very simple parsing because of how we will design the language, basically, we can always know what we are about to parse by looking at the current token.

Is it a `print`, great we are about to parse a print statement, is it a `if` ... well you get the point!
And no not all langs are like this,  consider Pythons list comps! 

`[x + 1]` vs `[x + 1 for x in range(10)]`, when parsing the `X` token you dont know if it is a list comp or a normal list you are parsing, you don't know that until you hit the `FOR` token. but we will be careful and avoid issues like this.

So, let's create our `parser.rs`!
```rust
// parsing/parser.rs
use anyhow::Context;
use thiserror::Error;

use crate::parsing::{
    ast,
    tokens::{Token, TokenType},
    StreamConsumer,
};

#[derive(Error, Debug)]
#[error("Parser error on line {line}, char {char}: expected one of {expected}, got {got:?}")]
pub struct ParsingError {
    pub line: usize,
    pub char: usize,
    pub got: TokenType,
    pub expected: String,
}
```
Our parsing errors are generally just going to be along the lines of "expected RIGHT_PAREN got PLUS" and stuff like that.
Or like "expected one of [SEMICOLON, PLUS] got EOF".

```rust
fn error(got: Token, expected: String) -> ParsingError {
    ParsingError {
        line: got.line,
        char: got.char,
        got: got._type,
        expected,
    }
}
```
Here we generate the error from the got token and the expected ones.
```rust
pub struct Parser(StreamConsumer<Token>);

impl Parser {
    pub fn new(tokens: Vec<Token>) -> Self {
        Self(StreamConsumer::new(tokens.into_iter().collect()))
    }

    fn peek(&self) -> anyhow::Result<&TokenType> {
        self.0
            .peek()
            .context("Unexpected End of Tokens")
            .map(|t| &t._type)
    }

    fn advance(&mut self) -> anyhow::Result<Token> {
        self.0.advance().context("Unexpected End of Tokens")
    }

    fn expect(&mut self, expected_token: TokenType) -> anyhow::Result<TokenType> {
        let token = self.advance()?;

        if token._type == expected_token {
            Ok(token._type)
        } else {
            Err(error(token, format!("{expected_token:?}")))?
        }
    }
}
```
This is the general structure of the parser, we are mainly going to be using `expect` to parse the different things that could show up!

let's start from the bottom and work up: 
```rust
    fn literal(&mut self) -> anyhow::Result<ast::Literal> {
        let token = self.advance()?;
        match token._type {
            TokenType::Integer(i) => Ok(ast::Literal::Integer(i)),
            _ => Err(error(token, "Literal".to_string()))?,
        }
    }

    fn expression(&mut self) -> anyhow::Result<ast::Expression> {
        match self.peek()? {
            TokenType::Integer(_) => Ok(ast::Expression::Literal(self.literal()?)),
            _ => Err(error(self.advance()?, "Expression".to_string()))?,
        }
    }

    fn statement(&mut self) -> anyhow::Result<ast::Statement> {
        let token = self.advance()?;
        match token._type {
            TokenType::Print => {
                let result = ast::Statement::Print(self.expression()?);
                self.expect(TokenType::SemiColon)?;
                Ok(result)
            }
            _ => Err(error(token, "Statement".to_string()))?,
        }
    }

    pub fn module(mut self) -> anyhow::Result<ast::Module> {
        let mut statements = Vec::new();

        while self.peek()? != &TokenType::Eof {
            statements.push(self.statement()?);
        }

        Ok(ast::Module(statements))
    }
```

It is really straightforward really, we check what we are currently at then call the needed function.
Let's add this to our `parse` function!

```rust
// parsing/mod.rs

pub fn parse(code: &str, compiler_options: &CompilerOptions) -> anyhow::Result<ast::Module> {
    let tokenizer = tokens::Tokenizer::new(code);
    let tokens = tokenizer.tokenize()?;

    if compiler_options.output_tokens {
      // ..
    }

    let parser = parser::Parser::new(tokens);
    let ast = parser.module()?;

    if compiler_options.output_ast {
        println!("AST: {ast:#?}");
    }

    Ok(ast)
}
```

We can now run our code on the same file
```bash
cargo run -- run test.viv --output-ast
```
And you should get something like:
```rust
Module(
    [
        Print(
            Literal(
                Integer(
                    1,
                ),
            ),
        ),
        Print(
            Literal(
                Integer(
                    2,
                ),
            ),
        ),
    ],
)
```

## Conclusion

Oh boy this got long, but now we should have a fully working parser generating our ast, In the next blog we will create the internal IR and type analyzer, they should be much simpler than this, so we might fit in the code generation as well! So stay tuned
